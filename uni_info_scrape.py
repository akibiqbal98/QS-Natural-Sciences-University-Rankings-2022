# -*- coding: utf-8 -*-
"""Uni_info_scrape.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p2GBpBCChhcRfjK1u2puFFl9PNgkPUKk
"""

# !pip install selenium
# !apt-get update # to update ubuntu to correctly run apt install
# !apt install chromium-chromedriver
# !cp /usr/lib/chromium-browser/chromedriver /usr/bin

import sys
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')

import json
from time import sleep

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.remote.webelement import WebElement
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


def create_driver_and_wait(waiting_time=10):
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')

    # Create an instance of Chrome
    driver = webdriver.Chrome(options=chrome_options)

    # wait for the page to load
    driver.implicitly_wait(waiting_time)
    wait = WebDriverWait(driver, waiting_time)

    # Create an instance of Chrome
    return driver, wait


def change_page(by, value):
    element: WebElement = wait.until(EC.presence_of_element_located((by, value)))
    driver.execute_script("arguments[0].click();", element)
    sleep(0.5)


def change_tab(by, value):
    element: WebElement = wait.until(EC.presence_of_element_located((by, value)))
    driver.execute_script("arguments[0].click();", element)
    sleep(0.5)


def get_texts_from_elements(elements: 'list|tuple[str,str]'):
    if isinstance(elements, list):
        return list(map(lambda x: x.text, elements))
    elif isinstance(elements, tuple):
        return list(map(lambda x: x.text, driver.find_elements(*elements)))


def get_attributes_from_elements(elements: 'list|tuple', attribute: str):
    if isinstance(elements, list):
        return list(map(lambda x: x.text, elements))
    elif isinstance(elements, tuple):
        return list(map(lambda x: x.get_attribute(attribute), driver.find_elements(*elements)))

# Create an instance of Chrome
driver, wait = create_driver_and_wait(waiting_time=10)

# Navigate to a website
url = "https://www.topuniversities.com/university-rankings/university-subject-rankings/2022/natural-sciences#university-rankings-indicators"
driver.get(url)

sleep(3)

ranks = []
uni_names = []
uni_links = []

TOTAL_PAGES = 51
for page in range(TOTAL_PAGES):
    try:
        print(f"Visiting page #{page: <2}", end=" ")

        CSS_SELECTOR_RANKS = 'div#ranking-data-load div._univ-rank.hide-this-in-mobile-indi'
        CSS_SELECTOR_LINKS = 'div#ranking-data-load a.uni-link'

        ranks.extend(get_texts_from_elements(By.CSS_SELECTOR, CSS_SELECTOR_RANKS))
        uni_names.extend(get_texts_from_elements(By.CSS_SELECTOR, CSS_SELECTOR_LINKS))
        uni_links.extend(get_attributes_from_elements(By.CSS_SELECTOR, CSS_SELECTOR_LINKS))

    except Exception as e:
        print(e)

    else:
        print(f"=> Completed")
        if page == TOTAL_PAGES-1:
            break
        change_page(By.CSS_SELECTOR,  'ul#alt-style-pagination a.page-link.next')


print(f"{f'  Scraped {TOTAL_PAGES} pages  ':~^30}", end="\n\n")
print("Total ranks: ", len(ranks))
print("Total uni names: ", len(uni_names))
print("Total uni links: ", len(uni_links), end="\n\n")


# Close the browser
sleep(3)
driver.quit()


uni_info = {}

# Create an instance of Chrome
driver, wait = create_driver_and_wait(waiting_time=10)

for index, rank, name, link in enumerate(zip(ranks, uni_names, uni_links)):
    print(f"{index: <3}Visiting : {name: <80}", end=" ")
    if name in uni_info:
        print(f"=> Already completed")
        continue

    uni_info[name] = {}

    driver.get(link)
    # ADMISSION PARTS
    CSS_SELECTOR_CATEGORIES = "div#admissionTab h4.univ-subsection-full-width-title"
    CSS_SELECTOR_VALUES = "div#admissionTab div.univ-subsection-full-width-div"

    admission_info_categories = get_texts_from_elements((By.CSS_SELECTOR, CSS_SELECTOR_CATEGORIES))
    admission_info_VALUES = driver.find_elements(By.CSS_SELECTOR, CSS_SELECTOR_VALUES)

    admission_values = {}
    for category, element in zip(admission_info_categories, admission_info_VALUES):
        labels = get_texts_from_elements(element.find_elements(
            By.CSS_SELECTOR, "div div.univ-subsection-full-width-value label"))
        values = get_texts_from_elements(element.find_elements(
            By.CSS_SELECTOR, "div div.univ-subsection-full-width-value div"))

        admission_values[category] = {k: v for k, v in zip(labels, values)}

    uni_info[name].update({'Admission': admission_values})

    # STUDENT AND STAFF PARTS
    try:
        change_tab(By.ID, 'studStaff_Tab')
    except Exception as e:
        print("ERROR!")
        print(e)
        uni_info[name].update({'Student and Staff': {}})
        continue
    else:
        CSS_SELECTOR_TITLE = "div#studStaffTab label.studstaff-subsection-title"
        titles = get_texts_from_elements((By.CSS_SELECTOR, CSS_SELECTOR_TITLE))

        CSS_SELECTOR_COUNT = "div#studStaffTab div.studstaff-subsection-count"
        counts = get_texts_from_elements((By.CSS_SELECTOR, CSS_SELECTOR_COUNT))

        uni_info[name].update({'Student and Staff': {k: v for k, v in zip(titles, counts)}})
        
        print(f"=> Completed")

print(f"{f'  Scraped {len(uni_info)} universities  ':~^30}", end="\n\n")

# Close the browser
sleep(3)
driver.quit()

"""ERRORS:
```
Weizmann Institute of Science
Academy of Sciences of the Czech Republic
Tata Institute of Fundamental Research
The University of Tennessee, Knoxville
International School for Advanced Studies of Trieste
The Scripps Research Institute (TSRI)
Universit√© de Savoie, Chambery, Annecy

```
"""

print(json.dumps(uni_info, indent=2))

json.dump(uni_info, open("uni_info.json", "w"), indent=2)

len(uni_info)

